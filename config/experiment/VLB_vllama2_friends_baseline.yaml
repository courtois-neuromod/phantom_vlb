# @package _global_

datamodule:
  _target_:
    src.datamodule.VLBDataModule
  config:
    _target_:
      src.datamodule.VLBDataModuleConfig
    lazyload_path: $SCRATCH_PATH/friends_llFile_${subject}_s*_n*.h5
    subject: ${subject}
    seasons: [s1, s2, s4, s5, s6]
    delay: 3
    window: 3    
    random_state: ${random_state}
    shuffle_val_data: False
    batch_size: 5
    num_workers: 39

litmodule:
  _target_:
    src.litmodule.VLBLitModule
  config:
    _target_:
      src.litmodule.VLBLitModuleConfig
    model_path: DAMO-NLP-SG/VideoLLaMA2-7B
    freeze_backbone: True
    use_lora: False
    lora_r: null
    lora_alpha: null
    lora_dropout: null    
    dropout_rate: 0.1
    num_target: 1000
    l2_lambda: 0.001
    lr: 1e-4
    betas: [0.9, 0.999]
    eps: 1e-08
    weight_decay: 1e-2
    lr_scheduler_name: CosineAnnealingLR
    last_epoch: -1
    t_max: 50000

trainer:
  _target_: lightning.pytorch.Trainer
  precision: "bf16-mixed"
  accelerator: "gpu"
  gradient_clip_val: 1
  devices: 1
  num_nodes: 1
  max_epochs: 10
  val_check_interval: 0.2
  log_every_n_steps: 15

exp_logger:
  _target_: lightning.pytorch.loggers.CometLogger
  api_key: ${my_api_key}
  workspace: ${my_workspace}
  project: phantom_mm
  name: vllama2_vlb_friends_baseline_${subject}

output_dir: ./results/videollama2/brain_finetune/friends/lightning_ckpt/baseline

name: vllama2_vlb_friends
